{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stude Terkel Radio Archive Scraper\n",
    "\n",
    "Downloads program information from [WFMT's archive](https://studsterkel.wfmt.com/explore#t=date) of radio programs hosted by Studs Terkel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Python tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import boto3\n",
    "import requests\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "from rich.progress import track\n",
    "from datetime import date\n",
    "from bs4 import BeautifulSoup\n",
    "from boto3.s3.transfer import S3Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download program list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://studsterkel.wfmt.com/explore#t=date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull all the segemented blocks on the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_list = soup.find_all(class_=\"prog_year_block\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through them and grab all the URLs, which lead to program pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in year_list:\n",
    "    a_list = year.find_all(\"a\")\n",
    "    link_list.extend([a['href'] for a in a_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that list is unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_links = set(link_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the total number of URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5023"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape program pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare functions to parse pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_meta(e):\n",
    "    \"\"\"\n",
    "    Parse a grid cell of metadata from the bottom of a program page.\n",
    "    \"\"\"\n",
    "    # Get all the p tags\n",
    "    p_list = e.find_all(\"p\")\n",
    "    d = {}\n",
    "    for p in p_list:\n",
    "        # Split out the bolded text as the label\n",
    "        label = p.strong.text\n",
    "        p.strong.extract()\n",
    "        # Keep the rest as the value\n",
    "        value = p.text.strip()\n",
    "        # Add to the dictionary\n",
    "        d[label] = value\n",
    "    # Return all dictionaries in this block\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url2name(url):\n",
    "    return pathlib.Path(urllib.parse.urlparse(url).path).stem + \".html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_html(url):\n",
    "    name = url2name(url)\n",
    "    path = pathlib.Path(\"./html\") / name\n",
    "    if path.exists():\n",
    "        return path\n",
    "    else:\n",
    "        headers = {\n",
    "            'User-Agent': 'Studs Terkel Radio Archive Scraper (github.com/palewire/studs-terkel-radio-feed/)',\n",
    "        }\n",
    "        r = requests.get(f\"https://studsterkel.wfmt.com{url}\", headers=headers)\n",
    "        if not r.status_code == 200:\n",
    "            print(f\"Failed with status code {r.status_code}\")\n",
    "            return None\n",
    "        with open(path, \"w\") as fh:\n",
    "            fh.write(r.text)\n",
    "        time.sleep(0.15)\n",
    "        return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through all URLs and scrape each individual page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Working... <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Working... \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for url in track(list(unique_links)):\n",
    "    d = download_html(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_program(path):\n",
    "    \"\"\"\n",
    "    Scrape a program page and return data.\n",
    "    \"\"\"\n",
    "    # Open the page\n",
    "    with open(path, \"r\") as fh:\n",
    "        html = fh.read()\n",
    "    \n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(html)\n",
    "    \n",
    "    # Pull out the title\n",
    "    title = soup.find(\"h1\").text\n",
    "    \n",
    "    # Parse out all metadata\n",
    "    meta = {}\n",
    "    for e in soup.find(class_=\"meta_data__section\").find_all(class_=\"col-4\"):\n",
    "        meta.update(parse_meta(e))\n",
    "    \n",
    "    # Grab the MP3 URL, if it exists\n",
    "    media = soup.find(class_=\"audio_trigger\")\n",
    "    if media:\n",
    "        mp3_url = media['data-track-url']\n",
    "    else:\n",
    "        mp3_url = None\n",
    "    \n",
    "    # Grab the synopsis, if it exists\n",
    "    summary = soup.find(class_=\"program_synopsis__body\")\n",
    "    if summary:\n",
    "        synopsis = summary.h2.text\n",
    "    else:\n",
    "        synopsis = None\n",
    "    \n",
    "    # Return the scraped data\n",
    "    return dict(\n",
    "        title=title,\n",
    "        mp3_url=mp3_url,\n",
    "        archive_url=\"/programs/\" + path.stem,\n",
    "        synopsis=synopsis,\n",
    "        **meta\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_list = list(pathlib.Path(\"./html\").glob(\"*.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Working... <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:00</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Working... \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "program_list = []\n",
    "for p in track(html_list):\n",
    "    d = scrape_program(p)\n",
    "    program_list.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(program_list).rename(columns={\n",
    "    \"Broadcast Date\": \"broadcast_date\",\n",
    "    \"Physical Format\": \"physical_format\",\n",
    "    \"Digital Format\": \"digital_format\",\n",
    "    \"Ownership\": \"ownership\",\n",
    "    \"Language\": \"language\",\n",
    "    \"Program Sponsor\": \"program_sponsor\",\n",
    "    \"Duration\": \"duration\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate extra columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(s):\n",
    "    if pd.isnull(s):\n",
    "        return None\n",
    "    try:\n",
    "        month, day, year = s.split(\" \")\n",
    "    except ValueError:\n",
    "        return None\n",
    "    return pd.to_datetime(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.archive_url = df.archive_url.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['broadcast_datetime'] = df.broadcast_date.apply(parse_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['broadcast_year'] = df.broadcast_datetime.dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['broadcast_month'] = df.broadcast_datetime.dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['broadcast_monthday'] = df.broadcast_datetime.dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['has_mp3_url'] = ~pd.isnull(df.mp3_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     2872\n",
       "False    2149\n",
       "Name: has_mp3_url, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.has_mp3_url.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values([\"broadcast_month\", \"broadcast_monthday\", \"broadcast_year\"]).to_csv(\"../data/programs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download mp3 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "sked = pd.read_csv(\"../data/schedule.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_url = sked[~pd.isnull(sked.archive_url)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not has_url.archive_url.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "programs = pd.read_csv(\"../data/programs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_df = has_url.merge(\n",
    "    programs,\n",
    "    on=\"archive_url\",\n",
    "    how=\"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_dir = os.path.abspath(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = os.path.dirname(this_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_mp3(mp3_url):\n",
    "    \"\"\"\n",
    "    Download the mp3 URL and return the local file path.\n",
    "    \"\"\"\n",
    "    # If there's no URL, there's no path\n",
    "    if not mp3_url or pd.isnull(mp3_url):\n",
    "        return None\n",
    "    \n",
    "    # Build the local path using the URL\n",
    "    filename = mp3_url.split(\"/\")[-1].replace(\"published%2F\", \"\")\n",
    "    filepath = pathlib.Path(\"./mp3\") / filename\n",
    "    \n",
    "    # If the file is already downloaded, we're good to go\n",
    "    if filepath.exists():\n",
    "        pass\n",
    "    # If not, download it\n",
    "    else:\n",
    "        print(f\"Downloading {mp3_url} to {filepath}\")\n",
    "        opener = urllib.request.build_opener()\n",
    "        opener.addheaders = [(\"Referer\", \"https://studsterkel.wfmt.com/\")]\n",
    "        urllib.request.install_opener(opener)\n",
    "        urllib.request.urlretrieve(mp3_url, filepath)\n",
    "        time.sleep(0.125)\n",
    "\n",
    "    # Return the path\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_df['mp3_path'] = feed_df.mp3_url.apply(download_mp3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload mp3 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_s3_client(bucket, region=\"us-east-1\"):\n",
    "    \"\"\"\n",
    "    Returns a transfer client ready to upload files to an s3 bucket.\n",
    "    Provide the S3 bucket name and region.\n",
    "    \"\"\"\n",
    "    credentials = { \n",
    "        'aws_access_key_id': os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "        'aws_secret_access_key': os.getenv('AWS_ACCESS_KEY_SECRET')\n",
    "    }\n",
    "    client = boto3.client('s3', region, **credentials)\n",
    "    return S3Transfer(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_df[pd.isnull(feed_df.mp3_url)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'studs-terkel-radio-archive-feed'\n",
    "client = _get_s3_client(bucket)\n",
    "for path in list(feed_df.mp3_path):\n",
    "    print(f\"Uploading {path} to {bucket}\")\n",
    "    client.upload_file(\n",
    "        path,\n",
    "        bucket,\n",
    "        path,\n",
    "        extra_args={'ACL': 'public-read', 'ContentType': 'audio/mpeg'}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_df['feed_url'] = feed_df.mp3_path.apply(lambda x: f'https://studs-terkel-radio-archive-feed.s3.amazonaws.com/{x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_df.sort_values([\"feed_date\"]).to_csv(\"../data/feed.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
